// This file provides the necessary LLMGateway.Core implementation
// These types would normally be provided by a NuGet package

namespace LLMGateway.Core.Models.Completion
{
    /// <summary>
    /// Represents a message in a conversation with an LLM
    /// </summary>
    public class Message
    {
        /// <summary>
        /// The role of the message sender (e.g., "system", "user", "assistant")
        /// </summary>
        public string Role { get; set; }

        /// <summary>
        /// The content of the message
        /// </summary>
        public string Content { get; set; }
    }

    /// <summary>
    /// Represents a request for a completion from an LLM model
    /// </summary>
    public class CompletionRequest
    {
        /// <summary>
        /// The model ID to use for completion
        /// </summary>
        public string ModelId { get; set; }

        /// <summary>
        /// The messages to include in the completion request
        /// </summary>
        public List<Message> Messages { get; set; }

        /// <summary>
        /// The maximum number of tokens to generate
        /// </summary>
        public int? MaxTokens { get; set; }

        /// <summary>
        /// The temperature to use for generation (higher = more random)
        /// </summary>
        public double? Temperature { get; set; }

        /// <summary>
        /// The top_p parameter for nucleus sampling
        /// </summary>
        public double? TopP { get; set; }        /// <summary>
        /// The number of completions to generate
        /// </summary>
        public int? N { get; set; }

        /// <summary>
        /// Whether to stream the response
        /// </summary>
        public bool Stream { get; set; }

        /// <summary>
        /// Sequences where the API will stop generating further tokens
        /// </summary>
        public List<string> Stop { get; set; }
    }

    /// <summary>
    /// Represents a choice in a completion response
    /// </summary>
    public class Choice
    {
        /// <summary>
        /// The index of the choice
        /// </summary>
        public int Index { get; set; }

        /// <summary>
        /// The message associated with this choice
        /// </summary>
        public Message Message { get; set; }

        /// <summary>
        /// The reason the generation stopped
        /// </summary>
        public string FinishReason { get; set; }
    }

    /// <summary>
    /// Represents token usage statistics for a completion
    /// </summary>
    public class CompletionUsage
    {
        /// <summary>
        /// The number of prompt tokens used
        /// </summary>
        public int PromptTokens { get; set; }

        /// <summary>
        /// The number of completion tokens used
        /// </summary>
        public int CompletionTokens { get; set; }

        /// <summary>
        /// The total number of tokens used
        /// </summary>
        public int TotalTokens { get; set; }
    }

    /// <summary>
    /// Represents a response from an LLM completion request
    /// </summary>
    public class CompletionResponse
    {
        /// <summary>
        /// The ID of the completion
        /// </summary>
        public string Id { get; set; }

        /// <summary>
        /// The object type
        /// </summary>
        public string Object { get; set; }

        /// <summary>
        /// The timestamp when the completion was created
        /// </summary>
        public long Created { get; set; }

        /// <summary>
        /// The model used for completion
        /// </summary>
        public string Model { get; set; }

        /// <summary>
        /// The choices generated by the model
        /// </summary>
        public List<Choice> Choices { get; set; }

        /// <summary>
        /// Usage statistics for the completion
        /// </summary>
        public CompletionUsage Usage { get; set; }
    }
}

namespace LLMGateway.Core.Interfaces
{
    using System.Threading.Tasks;
    using LLMGateway.Core.Models.Completion;

    /// <summary>
    /// Interface for services that provide completion capabilities from LLMs
    /// </summary>
    public interface ICompletionService
    {
        /// <summary>
        /// Creates a completion based on the provided request
        /// </summary>
        /// <param name="request">The completion request</param>
        /// <returns>The completion response</returns>
        Task<CompletionResponse> CreateCompletionAsync(CompletionRequest request);
    }
}
